# -*- coding: utf-8 -*-
"""Calculate Exploitability MFG_ODL23Vanguard_DDPG_MFC_cybersecurity_shared.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yYioFHe3gpz07BdVauuqiE1GV51NXh2A

**Code to solve a finite-state MFC problem with deep reinforcement learning to learn the value function and the control, as functions of the distribution.**

---

This code is provided for the sake of illustration.

Questions or comments: mathieu.lauriere@nyu.edu

**References:**
  - More details on this method and this example: Carmona, R., Laurière, M., & Tan, Z. (2019). Model-free mean-field reinforcement learning: mean-field MDP and mean-field Q-learning. To appear in Annals of Applied Probability. [arXiv preprint arXiv:1910.12802](https://arxiv.org/abs/1910.12802).
  - The deep RL algorithm implenented here is DDPG, and the code is based on the [Keras tutorial on DDPG](https://keras.io/examples/rl/ddpg_pendulum/).

# Setup
"""

import gym
from gym import spaces
import tensorflow as tf
from tensorflow.keras import layers
import numpy as np
import matplotlib.pyplot as plt
import time
import copy
import os
import csv

tf.config.threading.set_intra_op_parallelism_threads(4)
tf.config.threading.set_inter_op_parallelism_threads(4)

os.environ['TF_CPP_MIN_LOG_LEVEL'] = '0'
#tf.debugging.set_log_device_placement(True)
# Set seed for reproducibility
#set the seed according to time
random_seed = int(time.time())
np.random.seed(random_seed)

tf.random.set_seed(random_seed)

N_STATES_defenders = 4 # Number of states:  [DI, DS, UI, US]
N_STATES_attackers = 2
v_H_DEFAULT = 0.6

#examine if the code is run using tensorflow
gpus = tf.config.list_physical_devices('GPU')
if gpus:
    print("GPU is available")
else:
    print("GPU is not available")

"""# Environment"""

# ======================================== #
# ENVIRONMENT
# ======================================== #
action_upper_bound = 1.0
action_lower_bound = 0.0

# ======================================== #
# test if the files can be saved after running
# ======================================== #
save_dir = '/scratch/ca2699/DDPG-MFG/0905_experiment_results/tune_lr'
file_path = os.path.join(save_dir, 'example.txt')

# 创建并写入文件
with open(file_path, 'w') as f:
    f.write("This is a single line of text.")  # 写入一行文字

print(f"File saved to: {file_path}")

class CyberSecEnv(gym.Env):
  """Custom Environment that follows gym interface"""
  """Cyber security model"""

  def __init__(self, pop_distrib, T, dt, cn_sampler):
    super(CyberSecEnv, self).__init__()
    # Parameters
    self.T = T # for one time step
    self.dt = dt
    self.Nt = int(self.T/self.dt)
    self.Dt_inner_loop = self.dt
    self.Nt_inner_loop = 1 # for inner update of the distrib
    self.NS_defenders = N_STATES_defenders # number of states
    self.NS_attackers = N_STATES_attackers
    self.beta_UU = 0.3
    self.beta_UD = 0.4
    self.beta_DU = 0.3
    self.beta_DD = 0.4
    self.lambda_speed_defenders = 0.8 # speed of response
    self.lambda_speed_attackers = 0.8
    self.q_rec_D = 0.5
    self.q_rec_U = 0.4
    self.q_inf_D = 0.4
    self.q_inf_U = 0.3
    self.k_D = 0.3 # cost of being defended
    self.k_I = 0.5 # cost of being infected

    self.active = 0.4 #the cost for being active
    self.infect_defenders = 0.5 # the reward for infecting defenders

    self.common_noise_sampler = cn_sampler

    #the proportion of defenders be infected
    self.infected_defenders = 0
    #the proportion of attackers be active
    self.active_attackers = 0
    self.v_H = self.common_noise_sampler(v_H_DEFAULT)


    # Spaces must be gym.spaces objects
    self.action_space_defenders = spaces.Box(low=-action_lower_bound, high=action_upper_bound, dtype=np.float64, shape=(self.NS_defenders,))
    self.action_space_attackers = spaces.Box(low=-action_lower_bound, high = action_upper_bound, dtype=np.float64, shape=(self.NS_attackers,))
    self.observation_space_def = spaces.Box(low=0.0, high=1.0, dtype=np.float64, shape=(self.NS_defenders,))
    self.observation_space_att = spaces.Box(low=0.0, high=1.0, dtype=np.float64,shape=(self.NS_attackers,))

    # Population distribution
    self.pop_distrib_discrete_init = pop_distrib.copy()

    self.reset(self.pop_distrib_discrete_init)



  # =================== CONVERSION =================== #

  def get_proportion_infected_defenders(self,mu):
      self.infected_defenders = 0
      for iS in range(self.NS_defenders):
          if iS == self.get_index('DI') or iS == self.get_index('UI'):
              self.infected_defenders += mu[iS]
          else:
              continue

  def get_proportion_active_attackers(self,mu):
      self.active_attackers = 0
      #print('this is mu attackers',mu)
      for iS in range(self.NS_attackers):
          if iS == self.get_index('active'):
              self.active_attackers += mu[iS]

  def get_index(self, str):
    # Get index corresponding to a string
    if str=='DI':
        return 0
    if str=='DS':
        return 1
    if str=='UI':
        return 2
    if str=='US':
        return 3
    if str=='active':
          return 0
    if str=='inactive':
          return 1

  # =================== REWARD =================== #
  def running_cost_t_defenders(self, iS, mu):
      # running cost for given state and control
      rcost = 0
      if iS == self.get_index('DI') or iS == self.get_index('DS'):
          rcost += self.k_D*10
      if iS == self.get_index('DI') or iS == self.get_index('UI'):
          rcost += self.k_I*10
      return rcost


  def running_cost_t_attackers(self,iS,mu):
      a = 10
      b = 10
      #print('infected_defenders',self.infected_defenders)
      #not quite sure about whetehr this reward function for hte attackers is correct
      attacker_cost = 0
      #maybe need to add on some scalers before each term?
      attacker_cost += a*mu[0]- b*self.infected_defenders

      return attacker_cost


  # =================== DYNAMICS =================== #
  def get_lambda_t_continuousAlpha_defender(self, mu_t, alpha_t):
      # see page 656 of [Carmona, Delarue, volume I]
      lambda_matrix = np.zeros((self.NS_defenders, self.NS_defenders))
      lambda_matrix[self.get_index('DI'),self.get_index('DS')] = self.q_rec_D
      lambda_matrix[self.get_index('DS'),self.get_index('DI')] = self.v_H*self.q_inf_D + self.beta_DD*mu_t[self.get_index('DI')] + self.beta_UD*mu_t[self.get_index('UI')]
      lambda_matrix[self.get_index('UI'),self.get_index('US')] = self.q_rec_U
      lambda_matrix[self.get_index('US'),self.get_index('UI')] = self.v_H*self.q_inf_U + self.beta_UU*mu_t[self.get_index('UI')] + self.beta_DU*mu_t[self.get_index('DI')]
      alpha_t_trunc = alpha_t # already truncated by def of the environment
      #print(alpha_t_trunc)
      #print(self.lambda_speed_defenders)
      lambda_matrix[self.get_index('DI'),self.get_index('UI')] = alpha_t_trunc * self.lambda_speed_defenders
      lambda_matrix[self.get_index('DS'),self.get_index('US')] = alpha_t_trunc * self.lambda_speed_defenders
      lambda_matrix[self.get_index('UI'),self.get_index('DI')] = alpha_t_trunc * self.lambda_speed_defenders
      lambda_matrix[self.get_index('US'),self.get_index('DS')] = alpha_t_trunc * self.lambda_speed_defenders
      for iS in range(0,self.NS_defenders):
          lambda_matrix[iS, iS] = - np.sum(lambda_matrix[iS])
      return lambda_matrix

  def get_q_t_withActions_defender(self, mu_t, alpha):
      # see equation (7.37) in [Carmona, Delarue, volume I]
      #print('alpha[0] is', alpha[0])
      #print('alpha[1] is', alpha[1])
      q_t = np.zeros((self.NS_defenders, self.NS_defenders))
      for iS in range(self.NS_defenders):
          q_t[iS] = self.get_lambda_t_continuousAlpha_defender(mu_t, alpha[iS])[iS]
      return q_t

  def get_lambda_t_continousAlpha_attacker(self, mu_t,  alpha_t):
      lambda_matrix = np.zeros((self.NS_attackers, self.NS_attackers))

      lambda_matrix[self.get_index('active'),self.get_index('inactive')] = alpha_t * self.lambda_speed_attackers
      lambda_matrix[self.get_index('inactive'),self.get_index('active')] = alpha_t * self.lambda_speed_attackers

      for iS in range(0,self.NS_attackers):
          lambda_matrix[iS,iS] = -np.sum(lambda_matrix[iS])
      return lambda_matrix

  def get_q_t_withActions_attacker(self, mu_t,alpha):
      q_t = np.zeros((self.NS_attackers,self.NS_attackers))
      for iS in range(self.NS_attackers):
          q_t[iS] = self.get_lambda_t_continousAlpha_attacker(mu_t,alpha[iS])[iS]
      return q_t

  def get_mu_and_reward_defender(self, mu, alpha):
      # alpha = array of actions, one per state
      social_reward = 0 # reward for the social planner
      new_mu_prev = np.zeros(self.NS_defenders)
      new_mu = np.zeros(self.NS_defenders)
      new_mu_prev = mu
      for it in range(0,self.Nt_inner_loop):
          q_t = self.get_q_t_withActions_defender(new_mu_prev, alpha)
          new_mu = np.matmul(new_mu_prev, np.eye(self.NS_defenders) + self.Dt_inner_loop*q_t)
          self.running_cost_vec = np.zeros(self.NS_defenders)
          for iS in range(self.NS_defenders):
              self.running_cost_vec[iS] = self.running_cost_t_defenders(iS, new_mu_prev)
          social_reward += np.inner(new_mu, -self.running_cost_vec)*self.Dt_inner_loop
          new_mu_prev = new_mu
      return new_mu, social_reward

  def get_mu_and_reward_attacker(self, mu_attackers, alpha_attackers):
      social_reward_att = 0 # reward for the social planner

      #for the attackers
      new_mu_prev_attackers = np.zeros((self.NS_attackers,self.NS_attackers))
      new_mu_attackers = np.zeros((self.NS_attackers,self.NS_attackers))
      new_mu_prev_attackers = mu_attackers
      for it in range(0,self.Nt_inner_loop):
          q_t_attackers = self.get_q_t_withActions_attacker(new_mu_prev_attackers, alpha_attackers)
          #print('q_t_attacker is',q_t_attackers)
          new_mu_attackers = np.matmul(new_mu_prev_attackers, np.eye(self.NS_attackers) + self.Dt_inner_loop*q_t_attackers)
          social_reward_att -= self.running_cost_t_attackers(0, new_mu_attackers)*self.Dt_inner_loop
          new_mu_prev_attackers = new_mu_attackers
      #social_reward_att = - self.running_cost_t_attackers(0, new_mu_prev_attackers)
      return new_mu_attackers, social_reward_att

  def step(self, action):
    # Execute one time step within the environment
    mu_defender = self.state[0]
    mu_attacker = self.state[1]

    new_mu_defender, social_reward_defender = self.get_mu_and_reward_defender(mu_defender, action[0])
    new_mu_attacker, social_reward_attacker = self.get_mu_and_reward_attacker(mu_attacker, action[1])
    self.get_proportion_infected_defenders(mu_defender)
    self.get_proportion_active_attackers(mu_attacker)
    self.state[0] = new_mu_defender
    self.state[1] = new_mu_attacker
    social_reward = np.array([social_reward_defender, social_reward_attacker])
    self.t += self.dt
    done = (self.t > self.T)
    info = None
    self.v_H = self.active_attackers
    return self.state, social_reward, done, info


  def reset(self, pop_distrib):
    # Reset the state of the environment to an initial state
    self.t = 0.0
    self.state = copy.deepcopy(pop_distrib)
    self.get_proportion_infected_defenders(pop_distrib[0])
    self.get_proportion_active_attackers(pop_distrib[1])
    #print(self.state)
    return self.state

"""# Replay Buffer"""

# ======================================== #
# REPLAY BUFFER
# ======================================== #

# For experience replay
class Buffer:
    def __init__(self, num_states, num_actions, buffer_capacity=100000, batch_size=64):
        # Number of "experiences" to store at max
        self.buffer_capacity = buffer_capacity
        # Num of tuples to train on.
        self.batch_size = batch_size

        # Its tells us num of times record() was called.
        self.buffer_counter = 0

        array_Y = np.zeros((N_STATES_defenders,1))
        array_Z = np.zeros((N_STATES_attackers,1))

        array_X = np.array([array_Y,array_Z], dtype = object)


        # Instead of list of tuples as the exp.replay concept go
        # We use different np.arrays for each tuple element
        self.state_buffer = np.stack([array_X] * buffer_capacity, axis=0)
        self.action_buffer = np.stack([array_X]*buffer_capacity,axis=0)
        self.reward_buffer = np.zeros((self.buffer_capacity,2, 1))
        self.next_state_buffer = np.stack([array_X]*buffer_capacity,axis=0)

    # Takes (s,a,r,s') obervation tuple as input
    def record(self, obs_tuple):
        # Set index to zero if buffer_capacity is exceeded,
        # replacing old records
        index = self.buffer_counter % self.buffer_capacity

        for i_pop in range(0,2):
          #print(self.state_buffer[index])
          #print("obs_tuple = ", obs_tuple)
          self.state_buffer[index][i_pop] = obs_tuple[0][i_pop]
          #print("obs_tuple[1] = ", obs_tuple[1])
          self.action_buffer[index][i_pop] = obs_tuple[1][i_pop]
          self.reward_buffer[index][i_pop] = obs_tuple[2][i_pop]
          self.next_state_buffer[index][i_pop] = obs_tuple[3][i_pop]

        self.buffer_counter += 1

    # Eager execution is turned on by default in TensorFlow 2. Decorating with tf.function allows
    # TensorFlow to build a static graph out of the logic and computations in our function.
    # This provides a large speed up for blocks of code that contain many small TensorFlow operations such as this one.
    @tf.function
    def update(
        self, state_batch, action_batch, reward_batch, next_state_batch,
        actor_model_def, actor_optimizer_def, critic_model_def, critic_optimizer_def, target_actor_def, target_critic_def,
        actor_model_att, actor_optimizer_att, critic_model_att, critic_optimizer_att, target_actor_att, target_critic_att,
        gamma
    ):
        # Training and updating Actor & Critic networks.
        # See Pseudo Code.
          state_def_batch = [row[0] for row in state_batch]
          state_def_batch = tf.convert_to_tensor(state_def_batch)
          action_def_batch = [row[0] for row in action_batch]
          action_def_batch = tf.convert_to_tensor(action_def_batch)
          reward_def_batch = [row[0] for row in reward_batch]
          reward_def_batch = tf.convert_to_tensor(reward_def_batch)
          reward_def_batch = tf.cast(reward_def_batch, dtype=tf.float32)
          next_state_def_batch = [row[0] for row in next_state_batch]
          #print(next_state_def_batch)
          next_state_def_batch = tf.convert_to_tensor(next_state_def_batch)
          #print(next_state_def_batch)

          state_att_batch = [row[1] for row in state_batch]
          state_att_batch = tf.convert_to_tensor(state_att_batch)
          action_att_batch = [row[1] for row in action_batch]
          action_att_batch = tf.convert_to_tensor(action_att_batch)
          reward_att_batch = [row[1] for row in reward_batch]
          reward_att_batch = tf.convert_to_tensor(reward_att_batch)
          reward_att_batch = tf.cast(reward_att_batch, dtype=tf.float32)
          next_state_att_batch = [row[1] for row in next_state_batch]
          #print(next_state_att_batch)
          next_state_att_batch = tf.convert_to_tensor(next_state_att_batch)
          #print(next_state_att_batch)

          masked_tensor_def = tf.zeros_like(state_def_batch)
          masked_tensor_att = tf.zeros_like(state_att_batch)

          with tf.GradientTape() as tape:
              #print('hi',[next_state_def_batch,next_state_att_batch])
              next_state_def_batch=tf.reshape(next_state_def_batch,(self.batch_size,4))
              #print(next_state_def_batch)
              target_actions_def = target_actor_def([next_state_def_batch, masked_tensor_att], training=True)
              #print("target_actions defenders size= ", target_actions_def.shape)
              #print([next_state_def_batch,next_state_att_batch,target_actions_def])
              y = reward_def_batch + gamma * target_critic_def(
                  [next_state_def_batch, masked_tensor_att, target_actions_def], training=True
              )
              critic_def_value = critic_model_def([state_def_batch, masked_tensor_att, action_def_batch], training=True)
              critic_def_loss = tf.math.reduce_mean(tf.math.square(y - critic_def_value))

          critic_def_grad = tape.gradient(critic_def_loss, critic_model_def.trainable_variables)
          critic_optimizer_def.apply_gradients(
              zip(critic_def_grad, critic_model_def.trainable_variables)
          )

          with tf.GradientTape() as tape:
              actions_def = actor_model_def([state_def_batch,masked_tensor_att], training=True)
              critic_def_value = critic_model_def([state_def_batch, masked_tensor_att, actions_def], training=True)
              # Used `-value` as we want to maximize the value given
              # by the critic for our actions
              actor_def_loss = -tf.math.reduce_mean(critic_def_value)

          actor_def_grad = tape.gradient(actor_def_loss, actor_model_def.trainable_variables)
          actor_optimizer_def.apply_gradients(
              zip(actor_def_grad, actor_model_def.trainable_variables)
          )

          with tf.GradientTape() as tape:
              target_actions_att = target_actor_att([masked_tensor_def, next_state_att_batch], training=True)
              #print("target_actions = ", target_actions_2)
              y = reward_att_batch + gamma * target_critic_att(
                  [masked_tensor_def, next_state_att_batch, target_actions_att], training=True
              )
              critic_att_value = critic_model_att([masked_tensor_def, state_att_batch, action_att_batch], training=True)
              critic_att_loss = tf.math.reduce_mean(tf.math.square(y - critic_att_value))

          critic_att_grad = tape.gradient(critic_att_loss, critic_model_att.trainable_variables)
          critic_optimizer_att.apply_gradients(
              zip(critic_att_grad, critic_model_att.trainable_variables)
          )

          with tf.GradientTape() as tape:
              #print('hi you are here')
              #print([state_att_batch,state_att_batch])
              actions_att = actor_model_att([masked_tensor_def, state_att_batch], training=True)
              critic_att_value = critic_model_att([masked_tensor_def, state_att_batch, actions_att], training=True)
              # Used `-value` as we want to maximize the value given
              # by the critic for our actions
              actor_att_loss = -tf.math.reduce_mean(critic_att_value)

          actor_att_grad = tape.gradient(actor_att_loss, actor_model_att.trainable_variables)
          actor_optimizer_att.apply_gradients(
              zip(actor_att_grad, actor_model_att.trainable_variables)
          )

    # We compute the loss and update parameters
    def learn(self, actor_model_def, actor_optimizer_def, critic_model_def, critic_optimizer_def, target_actor_def, target_critic_def,
              actor_model_att, actor_optimizer_att, critic_model_att, critic_optimizer_att, target_actor_att, target_critic_att,
              gamma):
        # Get sampling range
          record_range = min(self.buffer_counter, self.buffer_capacity)
          # Randomly sample indices
          batch_indices = np.random.choice(record_range, self.batch_size)
          #print(batch_indices)
          #print(self.state_buffer[batch_indices])
          # Convert to tensors
          state_batch = self.state_buffer[batch_indices]
          action_batch = self.action_buffer[batch_indices]
          reward_batch = self.reward_buffer[batch_indices]
          reward_batch = tf.cast(reward_batch, dtype=tf.float32)
          next_state_batch = self.next_state_buffer[batch_indices]
          #print(next_state_batch)
          #list_of_lists_of_lists = [[arr.tolist() for arr in inner_list] for inner_list in outer_list]
          #print(state_batch[0])
          list_state_batch = [[arr.tolist() for arr in inner_list] for inner_list in state_batch]
          list_action_batch = [[arr.tolist() for arr in inner_list] for inner_list in action_batch]
          list_reward_batch = [[arr for arr in inner_list] for inner_list in reward_batch.numpy().tolist()]
          #print(reward_batch)
          list_next_state_batch = [[arr.tolist() for arr in inner_list] for inner_list in next_state_batch]
          #print(list_next_state_batch)

          self.update(list_state_batch, list_action_batch, list_reward_batch, list_next_state_batch, actor_model_def, actor_optimizer_def, critic_model_def, critic_optimizer_def, target_actor_def, target_critic_def,
                      actor_model_att, actor_optimizer_att, critic_model_att, critic_optimizer_att, target_actor_att, target_critic_att,
                      gamma)


# This update target parameters slowly
# Based on rate `tau`, which is much less than one.
@tf.function
def update_target(target_weights, weights, tau):
    for (a, b) in zip(target_weights, weights):
        a.assign(b * tau + a * (1 - tau))

"""# Neural Networks"""

# ======================================== #
# NEURAL NETWORKS
# ======================================== #

# Actor neural network
# Remark: range of the output should be adjusted depending on the example
def get_actor_def(num_states_def, num_states_att, num_actions):
    # Initialize weights between -3e-3 and 3-e3 to avoid saturating tanh
      last_init = tf.random_uniform_initializer(minval=-0.003, maxval=0.003)
      input_def = layers.Input(shape=(num_states_def,))
      input_att = layers.Input(shape=(num_states_att,))
      concatenated = layers.Concatenate()([input_def,input_att])
      out = layers.Dense(100, activation="sigmoid")(concatenated)
      out = layers.Dense(100, activation="sigmoid")(out)
      outputs = action_upper_bound * layers.Dense(N_STATES_defenders, activation="sigmoid", kernel_initializer=last_init)(out)
      model = tf.keras.Model([input_def,input_att], outputs)

      return model

def get_actor_att(num_states_def, num_states_att, num_actions):
    # Initialize weights between -3e-3 and 3-e3 to avoid saturating tanh
      last_init = tf.random_uniform_initializer(minval=-0.003, maxval=0.003)
      # Define two separate input layers
      input_def = layers.Input(shape=(num_states_def,))
      input_att = layers.Input(shape=(num_states_att,))

      concatenated = layers.Concatenate()([input_def, input_att])
      out = layers.Dense(100, activation="sigmoid")(concatenated)
      out = layers.Dense(100, activation="sigmoid")(out)
      outputs = action_upper_bound * layers.Dense(N_STATES_attackers, activation="sigmoid", kernel_initializer=last_init)(out)
      model = tf.keras.Model([input_def,input_att], outputs)

      return model

# Critic neural network
def get_critic(num_states_def, num_states_att, num_actions):
      state_def_input = layers.Input(shape=(num_states_def,))
      state_att_input = layers.Input(shape=(num_states_att,))
      action_input = layers.Input(shape=(num_actions))
      # Both could be passed through seperate layers before concatenating
      concat = layers.Concatenate()([state_def_input, state_att_input, action_input])
      out = layers.Dense(100, activation="sigmoid")(concat)
      out = layers.Dense(100, activation="sigmoid")(out)
      outputs = layers.Dense(1)(out)
      # Outputs single value for give state-action
      model = tf.keras.Model([state_def_input, state_att_input,  action_input], outputs)

      return model

"""# Policy computation"""

# Returns an action sampled from our Actor network plus some noise for exploration.
def policy(state_def, state_att,  actor_model, noise_object, action_lower_bound, action_upper_bound, mask = True, player_index = None):
    #print([state_def, state_att])
      if mask:
          if player_index == 0:
            mask_state_vec = tf.zeros_like(state_att)
            input_vec = [state_def, mask_state_vec]
            sampled_actions = tf.squeeze(actor_model(input_vec))
          
          elif player_index ==1:
            mask_state_vec = tf.zeros_like(state_def)
            input_vec = [mask_state_vec, state_att]
            sampled_actions = tf.squeeze(actor_model(input_vec))
      
      else:    
        sampled_actions = tf.squeeze(actor_model([state_def,state_att]))
      
      noise = noise_object()
      # Adding noise to action
      #print('the sampled actions is', sampled_actions)
      #print('the noise is',noise)
      sampled_actions_noisy = sampled_actions.numpy() + noise
      #print('the sampled actions noisy is', sampled_actions_noisy)
      # We make sure action is within bounds
      legal_action = np.clip(sampled_actions_noisy, action_lower_bound, action_upper_bound)
      return [np.squeeze(legal_action)]

"""# Exploration noise"""

# ======================================== #
# HYPERPARAMETERS FOR TRAINING
# ======================================== #

# Ornstein-Uhlenbeck process for noisy perturbations to ensure exploration by the Actor network
class OUActionNoise:
    def __init__(self, mean, std_deviation, theta=0.15, dt=1e-2, x_initial=None):
        self.theta = theta
        self.mean = mean
        self.std_dev = std_deviation
        self.dt = dt
        self.x_initial = x_initial
        self.reset()

    def __call__(self):
        # Formula taken from https://www.wikipedia.org/wiki/Ornstein-Uhlenbeck_process.
        x = (
            self.x_prev
            + self.theta * (self.mean - self.x_prev) * self.dt
            + self.std_dev * np.sqrt(self.dt) * np.random.normal(size=self.mean.shape)
        )
        # Store x into x_prev
        # Makes next noise dependent on current one
        self.x_prev = x
        return x

    def reset(self):
        if self.x_initial is not None:
            self.x_prev = self.x_initial
        else:
            self.x_prev = np.zeros_like(self.mean)

"""Then we need to calculate exploitability."""

#the learn DDPG class we use to train the networks in compute max J function
def DDPG_learn(training_actor_model_1, training_actor_optimizer_1, training_critic_model_1, training_critic_optimizer_1,
               training_target_actor_1, training_target_critic_1,
               state_1, state_2,
               next_state_1, next_state_2,
               action_object, training_reward,
               gamma):
          with tf.GradientTape() as tape:
              target_actions_1 = training_target_actor_1([next_state_1, next_state_2], training=True)
              y = training_reward + gamma * training_target_critic_1(
                  [next_state_1, next_state_2, target_actions_1], training=True
              )
              #print(state.shape)
              #print(action_object.shape)
              critic_1_value = training_critic_model_1([state_1,state_2,action_object], training=True)
              #print(critic_value)
              critic_1_loss = tf.math.reduce_mean(tf.math.square(y - critic_1_value))
              #print(critic_loss)
          #training_critic_model.summary()
          critic_1_grad = tape.gradient(critic_1_loss, training_critic_model_1.trainable_variables)
          training_critic_optimizer_1.apply_gradients(
              zip(critic_1_grad, training_critic_model_1.trainable_variables)
          )

          with tf.GradientTape() as tape:
              actions_1 = training_actor_model_1([state_1,state_2], training=True)
              critic_1_value = training_critic_model_1([state_1, state_2,actions_1], training=True)
              # Used `-value` as we want to maximize the value given
              # by the critic for our actions
              actor_1_loss = -tf.math.reduce_mean(critic_1_value)

          actor_1_grad = tape.gradient(actor_1_loss, training_actor_model_1.trainable_variables)
          '''for grad, var in zip(actor_grad, training_actor_model.trainable_variables):
            print(f"Gradient shape: {grad.shape}, Variable shape:{var.shape}")'''
          training_actor_optimizer_1.apply_gradients(
              zip(actor_1_grad, training_actor_model_1.trainable_variables)
          )

from datetime import datetime
def compute_maxJ(object_actor_model_1, object_critic_model_1, object_target_actor_1, object_target_critic_1, target_actor_2,
                                 gamma,
                                 training_env,
                                 ou_noise_zero,
                                 ep,
                                 distrib_init_sampler,
                                 distrib_init_sampler_from_list,
                                 distrib_init_list,
                                 actor_lr,
                                 critic_lr,
                                 std_dev_ou_noise,
                                 tau,
                                 eval_times = 11, object_index = 0, other = 1):

  if object_index == 0:
    actor_model_object = get_actor_def(N_STATES_defenders,N_STATES_attackers, N_STATES_defenders)
    critic_model_object = get_critic(N_STATES_defenders,N_STATES_attackers, N_STATES_defenders)
    target_actor_object = get_actor_def(N_STATES_defenders,N_STATES_attackers, N_STATES_defenders)
    target_critic_object = get_critic(N_STATES_defenders,N_STATES_attackers, N_STATES_defenders)

  elif object_index == 1:
    actor_model_object = get_actor_att(N_STATES_defenders,N_STATES_attackers, N_STATES_attackers)
    critic_model_object = get_critic(N_STATES_defenders,N_STATES_attackers, N_STATES_attackers)
    target_actor_object = get_actor_att(N_STATES_defenders,N_STATES_attackers, N_STATES_attackers)
    target_critic_object = get_critic(N_STATES_defenders,N_STATES_attackers, N_STATES_attackers)

  #set the weights for the models at the current episode
  target_actor_object.set_weights(object_target_actor_1.get_weights())
  target_critic_object.set_weights(object_target_critic_1.get_weights())
  actor_model_object.set_weights(object_actor_model_1.get_weights())
  critic_model_object.set_weights(object_critic_model_1.get_weights())

  pop_distrib_reset = distrib_init_sampler()

  fresh_buffer = Buffer(num_states = 4, num_actions = 4, buffer_capacity=5000,batch_size=128)

  #print(critic_lr)
  actor_lr = actor_lr
  critic_lr = critic_lr
  fresh_actor_optimizer = tf.keras.optimizers.Adam(actor_lr)
  fresh_critic_optimizer = tf.keras.optimizers.Adam(critic_lr)

  maxJ = 0
  std_dev_ou_noise = std_dev_ou_noise

  inner_ddpg_training_reward_list = []

  current_time = datetime.now().strftime("%Y%m%d-%H%M%S")
  if object_index == 0:
    log_dir = f"logs/calculate_exploitability/player_{'Defender'}_stv_{std_dev_ou_noise}_att_lr_{actor_lr}_critic_lr_{critic_lr}_{current_time}_episode{ep}"
  elif object_index == 1:
    log_dir = f"logs/calculate_exploitability/player_{'Attacker'}_stv_{std_dev_ou_noise}_att_lr_{actor_lr}_critic_lr_{critic_lr}_{current_time}_episode{ep}"
  summary_writer = tf.summary.create_file_writer(log_dir)
 #emporarily change to 1 for debugging

  save_dir = '/scratch/ca2699/DDPG-MFG/0905_experiment_results/tune_lr'

  for i in range(400):
    print('this is inner episode i',i)
    ou_noise_zero = OUActionNoise(mean = np.zeros(1), std_deviation = float(std_dev_ou_noise)*np.zeros(1))
    ou_noise_defenders = OUActionNoise(mean = np.zeros((N_STATES_defenders)), std_deviation = float(std_dev_ou_noise)*np.ones((N_STATES_defenders)))
    ou_noise_attackers = OUActionNoise(mean = np.zeros((N_STATES_attackers)), std_deviation = float(std_dev_ou_noise)*np.ones((N_STATES_attackers)))
    #question: do I need to come back to the same initial distribution at the beginning of every episode?
    #pop_distrib_reset = distrib_init_sampler()
    #print('the pop distrib reset is', pop_distrib_reset)
    pop_distrib_reset = distrib_init_list[1]
    prev_state = training_env.reset(pop_distrib_reset)
    print('here we reset the prev state as',prev_state)
    inner_ddpg_training_reward = 0
    initial_state = copy.deepcopy(prev_state)
    pop_distrib_lst = [initial_state]
    pop_action_lst_def = []
    pop_action_lst_att = []

    while True:
      #print(training_env.t)
      prev_state_def = prev_state[0]
      prev_state_att = prev_state[1]
      tf_prev_state_def = tf.expand_dims(tf.convert_to_tensor(prev_state_def),0)
      tf_prev_state_att = tf.expand_dims(tf.convert_to_tensor(prev_state_att),0)
      if object_index == 0:
          action_object = policy(tf_prev_state_def, tf_prev_state_att, actor_model_object, ou_noise_defenders, action_lower_bound, action_upper_bound, mask = False)
      elif object_index == 1:
          action_object = policy(tf_prev_state_def, tf_prev_state_att, actor_model_object, ou_noise_attackers, action_lower_bound, action_upper_bound, mask = False)

      #print('the previous state is', prev_state)
      action_1 = policy(tf_prev_state_def, tf_prev_state_att, target_actor_2, ou_noise_zero, action_lower_bound, action_upper_bound, mask = False)

      action_order = [0.0,0.0]
      action_order[object_index] = action_object[0]
      action_order[other] = action_1[0]
      action = np.array(action_order, dtype=object)

      #print('the action is',action_object)
      if object_index == 0:
        pop_action_lst_def.append(action_object)
        pop_action_lst_att.append(action_1)
      elif object_index == 1:
        pop_action_lst_def.append(action_1)
        pop_action_lst_att.append(action_object)

      #print('defender action is', pop_action_lst_def)
      #print('attacker action is', pop_action_lst_att)
      state,reward, done, info = training_env.step(action)

      prev_state = [np.array(element) for element in prev_state ]
      state = [np.array(element) for element in state]

      #print('the reward is',reward)

      inner_ddpg_training_reward += reward[object_index]

      fresh_buffer.record([prev_state,action,reward,state])

      record_range = min(fresh_buffer.buffer_counter, fresh_buffer.buffer_capacity)

      batch_indices = np.random.choice(record_range, buffer.batch_size)

      state_batch = fresh_buffer.state_buffer[batch_indices]
      action_batch = fresh_buffer.action_buffer[batch_indices]
      reward_batch = fresh_buffer.reward_buffer[batch_indices]
      next_state_batch = fresh_buffer.next_state_buffer[batch_indices]
      #print(type(state_batch))
      first_player_state_batch = tf.convert_to_tensor([list(array[0]) for array in state_batch]) #ML: Maybe important: Should 0 be replaced by object_index?
      first_player_action_batch = tf.convert_to_tensor([list(array[0]) for array in action_batch])
      first_player_reward_batch = tf.convert_to_tensor([list(array[0]) for array in reward_batch])
      first_player_reward_batch = tf.cast(first_player_reward_batch, dtype=tf.float32)
      next_first_player_state_batch = tf.convert_to_tensor([list(array[0]) for array in next_state_batch])

      second_player_state_batch = tf.convert_to_tensor([list(array[1]) for array in state_batch]) #ML: Should 1 be replaced by other_1?
      second_player_action_batch = tf.convert_to_tensor([list(array[1]) for array in action_batch])
      second_player_reward_batch = tf.convert_to_tensor([list(array[1]) for array in reward_batch])
      second_player_reward_batch = tf.cast(second_player_reward_batch, dtype=tf.float32)
      next_second_player_state_batch = tf.convert_to_tensor([list(array[1]) for array in next_state_batch])

      object_action_batch = tf.convert_to_tensor([list(array) for array in action_batch[:, object_index]])
      object_reward_batch = tf.convert_to_tensor(reward_batch[:,object_index])
      object_reward_batch = tf.cast(object_reward_batch, dtype = tf.float32)

      #print(first_player_state_batch.size())
      #print(second_player_state_batch.size())

      #print(state_batch.size())
      DDPG_learn(actor_model_object, fresh_actor_optimizer, critic_model_object, fresh_critic_optimizer,
                 target_actor_object, target_critic_object,
                 first_player_state_batch, second_player_state_batch,
                 next_first_player_state_batch, next_second_player_state_batch,
                 object_action_batch,
                 object_reward_batch,
                 gamma)
      update_target(target_actor_object.variables, actor_model_object.variables,tau)
      update_target(target_critic_object.variables,critic_model_object.variables, tau)

      if done:
        break
      pop_distrib_lst.append(state)
      prev_state= state

    #print('the inner ddpg training reward is',inner_ddpg_training_reward)
    inner_ddpg_training_reward_list.append(inner_ddpg_training_reward)

    with summary_writer.as_default():
          tf.summary.scalar('Inner Episode Reward',inner_ddpg_training_reward, step=i)
    #plot the distribution of population and actions
    # the state distribution of defenders
    if (i%50)==0:
      t_space = np.linspace(0,training_env.T,num=training_env.Nt+1,endpoint=True)
      pop_distrib_lst_def = [sublist[0] for sublist in pop_distrib_lst]
      #print(pop_distrib_lst_def)
      pop_distrib_array_def = np.asarray(pop_distrib_lst_def)
      #print(pop_distrib_array)
      first_column = [array[0] for array in pop_distrib_array_def]
      second_column = [array[1] for array in pop_distrib_array_def]
      third_column = [array[2] for array in pop_distrib_array_def]
      fourth_column = [array[3] for array in pop_distrib_array_def]

      plt.plot(t_space, first_column, label="DI")
      plt.plot(t_space, second_column, label="DS")
      plt.plot(t_space, third_column, label="UI")
      plt.plot(t_space, fourth_column, label="US")
      plt.xlabel("t")
      plt.ylabel("distribution")
      plt.legend()
      save_path = os.path.join(save_dir,"The_Second_distribution_defenders_inner_DDPG_episode{}_train-distrib.pdf".format(i))
      plt.savefig(save_path)
      plt.show()
      plt.clf()

      #the action distribution of defenders
      t_space = np.linspace(0, training_env.T, num = training_env.Nt+1, endpoint = True)
      pop_action_lst_def = [sublist[0] for sublist in pop_action_lst_def]
      pop_action_array_def = np.asarray(pop_action_lst_def)
      #print('the pop action array is',pop_action_array)
      first_column = [array[0] for array in pop_action_array_def]
      second_column = [array[1] for array in pop_action_array_def]
      third_column = [array[2] for array in pop_action_array_def]
      fourth_column = [array[3] for array in pop_action_array_def]

      plt.plot(t_space, first_column, label="DI")
      plt.plot(t_space, second_column, label="DS")
      plt.plot(t_space, third_column, label="UI")
      plt.plot(t_space, fourth_column, label="US")

      plt.xlabel("t")
      plt.ylabel("action")
      plt.legend()
      save_path = os.path.join(save_dir, "The_Second_actions_defenders_inner_DDPG_episode{}_train-distrib.pdf".format(i))
      plt.savefig(save_path)
      plt.show()
      plt.clf()

      #the state distribution of attackers
      t_space = np.linspace(0,training_env.T,num=training_env.Nt+1,endpoint=True)
      pop_distrib_lst_att = [sublist[1] for sublist in pop_distrib_lst]
      #print(pop_distrib_lst)
      pop_distrib_array_att = np.asarray(pop_distrib_lst_att)
      #print(pop_distrib_array)
      first_column = [array[0] for array in pop_distrib_array_att]
      second_column = [array[1] for array in pop_distrib_array_att]
      first_column = np.array(first_column)
      second_column = np.array(second_column)
      #print(first_column)
      plt.plot(t_space, first_column, label="active")
      plt.plot(t_space, second_column, label="inactive")
      plt.xlabel("t")
      plt.ylabel("distribution")
      plt.legend()
      save_path = os.path.join(save_dir, "The_Second_distribution_attacker_inner_DDPG_episode{}_train-distrib.pdf".format(i))
      plt.savefig(save_path)
      plt.show()
      plt.clf()

      #the action distribution of attackers
      t_space = np.linspace(0, training_env.T, num = training_env.Nt+1, endpoint = True)

      #print(pop_action_lst)
      pop_action_lst_att = [sublist[0] for sublist in pop_action_lst_att]
      pop_action_array_att = np.asarray(pop_action_lst_att)
      first_column = [array[0] for array in pop_action_array_att]
      second_column = [array[1] for array in pop_action_array_att]

      plt.plot(t_space, first_column, label="active")
      plt.plot(t_space, second_column, label="inactive")

      plt.xlabel("t")
      plt.ylabel("action")
      plt.legend()

      save_path = os.path.join(save_dir, "The_Second_actions_attacker_inner_DDPG_episode{}_train-distrib.pdf".format(i))
      plt.savefig(save_path)
      plt.show()
      plt.clf()



  plt.figure()
  plt.plot(inner_ddpg_training_reward_list)
  plt.xlabel('inner episode')
  plt.ylabel('reward')
  plt.title('random weights for other-inner DDOG reward training')

  plt.tight_layout()
  if object_index == 0:
    save_path = os.path.join(save_dir, 'The_Second_calculate_exploitability_exploitability_inner_reward_defender'+'reward-training_inner_ddpg_episodde{}.pdf'.format(ep))
    plt.savefig(save_path)
  elif object_index == 1:
    save_path = os.path.join(save_dir, 'The_Second_calculate_exploitability_exploitability_inner_reward_attacker'+'reward-training_inner_ddpg_episodde{}.pdf'.format(ep))
    plt.savefig(save_path)
  plt.show()
  plt.clf()
  episode_rewards_object = []

  #evaluate the target actor in several MC trajectories
  #for j in range(len(distrib_init_list)):
  for i in range(eval_times):
      pop_distrib_reset = distrib_init_list[1]
      prev_state = training_env.reset(pop_distrib_reset)
      #print(prev_state)
      #prev_state_def = prev_state[0]
      #prev_state_att = prev_state[1]
      episode_J_object = []

      while True:
        prev_state_def = prev_state[0]
        prev_state_att = prev_state[1]
        tf_prev_state_def = tf.expand_dims(tf.convert_to_tensor(prev_state_def),0)
        tf_prev_state_att = tf.expand_dims(tf.convert_to_tensor(prev_state_att),0)

        #print(tf_prev_state_def,tf_prev_state_att)

        if object_index == 0:
            action_object = policy(tf_prev_state_def, tf_prev_state_att, actor_model_object, ou_noise_defenders, action_lower_bound, action_upper_bound, mask = False)
        elif object_index == 1:
            action_object = policy(tf_prev_state_def, tf_prev_state_att, actor_model_object, ou_noise_attackers, action_lower_bound, action_upper_bound, mask = False)
        action_1 = policy(tf_prev_state_def,tf_prev_state_att, target_actor_2, ou_noise_zero, action_lower_bound, action_upper_bound, mask = False)
        action_order = [0.0,0.0]
        action_order[object_index] = action_object[0]
        action_order[other] = action_1[0]
        action = np.array(action_order,dtype=object)
        #print(action)
        state, reward, done, info = training_env.step(action)

        episode_J_object.append(reward)

        maxJ += reward[object_index]

        if done:
          episode_rewards_object.append(episode_J_object)

          break
        prev_state = state

  maxJ /= eval_times
 # maxJ /= len(distrib_init_list)
  print('maxJ is',maxJ)
  return maxJ

def compute_J_i (target_actor_1,
                 target_actor_2,
                 training_env,
                 distrib_init_sampler,
                 distrib_init_sampler_from_list,
                 distrib_init_list,
                 ou_noise_zeros,
                 eval_times = 5):
  J_1 = 0
  J_2 = 0
  episode_rewards_1 = []
  #for j in range(len(distrib_init_list)):
  for i in range(eval_times):
      pop_distrib_reset = distrib_init_list [1]
      prev_test_state = training_env.reset(pop_distrib_reset)

      while True:
        tf_prev_test_state_1 = tf.expand_dims(tf.convert_to_tensor(prev_test_state[0]),0)
        tf_prev_test_state_2 = tf.expand_dims(tf.convert_to_tensor(prev_test_state[1]),0)
        action_test_1 = policy(tf_prev_test_state_1, tf_prev_test_state_2, target_actor_1, ou_noise_zeros, action_lower_bound, action_upper_bound, player_index = 0)
        action_test_2 = policy(tf_prev_test_state_1, tf_prev_test_state_2, target_actor_2, ou_noise_zeros, action_lower_bound, action_upper_bound, player_index = 0)
        action_test = np.array([action_test_1[0], action_test_2[0]], dtype = object)

        #receive state and reward from environment
        state_test, reward_test, done_test, into = training_env.step(action_test)

        J_1 += reward_test[0]
        J_2 += reward_test[1]

        if done_test:
          break

        prev_test_state = state_test
  J_1 /= eval_times
  #J_1 /= len(distrib_init_list)
  J_2 /= eval_times
  #J_2 /= len(distrib_init_list)

  return J_1,J_2

"""# Training function"""

# ======================================== #
# TRAINING THE POLICY GIVEN THE DISTRIBUTION
# ======================================== #
def train_policy(training_env, actor_model_def, actor_optimizer_def, critic_model_def, critic_optimizer_def, target_actor_def, target_critic_def,
                 actor_model_att, actor_optimizer_att, critic_model_att, critic_optimizer_att, target_actor_att, target_critic_att,
                 gamma, tau_def, tau_att, buffer, N_total_episodes, distrib_init_sampler, distrib_init_sampler_from_list, distrib_init_list, actor_lr_lst_def,critic_lr_lst_def,sdv_lst_def, actor_lr_lst_att,critic_lr_lst_att,sdv_lst_att):
  # To store reward history of each episode
  ep_reward_list_def = []
  avg_reward_list_def = []
  ep_reward_list_att = []
  avg_reward_list_att = []

  ep_reward_list = []
  avg_reward_list = []

  #add the list for J functions
  ep_exploitability_list = []
  ep_exploitability_list_def = []
  ep_exploitability_list_att = []
  ep_max_J_def_list = []
  ep_max_J_att_list = []
  ep_J_def_list = []
  ep_J_att_list = []
  ep_test_reward_list = []

  # Action bounds
  action_lower_bound_def = training_env.action_space_defenders.low[0]
  action_upper_bound_def = training_env.action_space_defenders.high[0]
  action_lower_bound_att = training_env.action_space_attackers.low[0]
  action_upper_bound_att = training_env.action_space_attackers.high[0]

  start_time_policy_training = time.time()

  #std_dev_ou_noise = 0.2

  for ep in range(N_total_episodes+1):
    print('this is episode', ep)
    gpus = tf.config.list_physical_devices('GPU')
    if gpus:
        print("GPU is available")
    else:
        print("GPU is not available")
    # Noise in the policy for exploration
    #std_dev_ou_noise = 0.001
    std_dev_ou_noise = 0.08
    prev_state = training_env.reset(distrib_init_sampler())
    prev_state = [np.array(element) for element in prev_state ]
    pop_distrib_list = [prev_state]
    episodic_reward_def = 0
    episodic_reward_att = 0
    episodic_reward_target = 0
    ou_noise_defenders = OUActionNoise(mean=np.zeros(1), std_deviation=float(std_dev_ou_noise) * np.ones(1))
    ou_noise_attackers = OUActionNoise(mean=np.zeros(1), std_deviation=float(std_dev_ou_noise) * np.ones(1))
    while True: # loop over time steps in the current episode
        prev_state_def = prev_state[0]
        prev_state_att = prev_state[1]  
        tf_prev_state_def = tf.expand_dims(tf.convert_to_tensor(prev_state_def), 0)
        tf_prev_state_att = tf.expand_dims(tf.convert_to_tensor(prev_state_att), 0)


        action_def = policy(tf_prev_state_def, tf_prev_state_att,  actor_model_def  , ou_noise_defenders, action_lower_bound_def, action_upper_bound_def, player_index = 0)
        action_att = policy(tf_prev_state_def, tf_prev_state_att,  actor_model_att  , ou_noise_attackers, action_lower_bound_att, action_upper_bound_att, player_index = 1)
        #print([action_def, action_att])
        action = np.array([action_def[0], action_att[0]], dtype = object)
        #print(action)
        ## Recieve state and reward from environment.
        state, reward, done, info = training_env.step(action)
        prev_state = [np.array(element) for element in prev_state ]
        state = [np.array(element) for element in state]
        #print(prev_state)
        buffer.record((prev_state, action, reward, state))
        episodic_reward_def += reward[0]
        episodic_reward_att += reward[1]

        buffer.learn(actor_model_def, actor_optimizer_def, critic_model_def, critic_optimizer_def, target_actor_def, target_critic_def,
                     actor_model_att,actor_optimizer_att, critic_model_att, critic_optimizer_att, target_actor_att, target_critic_att,
                     gamma)

        update_target(target_actor_def.variables, actor_model_def.variables, tau_def)
        update_target(target_actor_att.variables, actor_model_att.variables, tau_att)
        update_target(target_critic_def.variables, critic_model_def.variables, tau_def)
        update_target(target_critic_att.variables, critic_model_att.variables, tau_att)
        # End this episode when `done` is True
        if done:
            break

        pop_distrib_list.append(state)

        prev_state = state

    ou_noise_zero = OUActionNoise(mean = np.zeros(1), std_deviation = float(std_dev_ou_noise)*np.zeros(1))
    if (ep % 50 == 0):
      # Plot result of latest training step
          save_dir = '/scratch/ca2699/DDPG-MFG/0905_experiment_results/tune_lr'

          print("Episode: {}, Train distribution".format(ep))
          t_space = np.linspace(0,training_env.T,num=training_env.Nt+1,endpoint=True)
          pop_distrib_list = [sublist[0] for sublist in pop_distrib_list]
          print('this is the pop_distrib_list',pop_distrib_list)
          pop_distrib_array = np.asarray(pop_distrib_list)
          #print(pop_distrib_array)
          for array in pop_distrib_array:
              #print(array[0])
              pass
          first_column = [array[0] for array in pop_distrib_array]
          second_column = [array[1] for array in pop_distrib_array]
          third_column = [array[2] for array in pop_distrib_array]
          fourth_column = [array[3] for array in pop_distrib_array]
          #print('this is first column',first_column)
          #print('this is second column',second_column)
          #print('this is third column',third_column)
          #print('this is fourth column',fourth_column)
          plt.plot(t_space, first_column, label="DI")
          plt.plot(t_space, second_column, label="DS")
          plt.plot(t_space, third_column, label="UI")
          plt.plot(t_space, fourth_column, label="US")
          plt.xlabel("t")
          plt.ylabel("distribution")
          plt.legend()
          save_path = os.path.join(save_dir, "The_Second_calculate_exploitability_distribution_defenders_episode{}_train-distrib.pdf".format(ep))
          plt.savefig(save_path)
          plt.show()
          plt.clf()
          #plot the state of the attackers
          t_space = np.linspace(0,training_env.T,num=training_env.Nt+1,endpoint=True)
          #first_column_att = [array[1][0] for array in pop_distrib_array]
          #second_column_att = [array[1][1] for array in pop_distrib_array]
          #plt.plot(t_space,first_column_att,label='active')
          #plt.plot(t_space,second_column_att,label='inactive')
          #plt.xlabel('t')
          #plt.ylabel('distribution')
          #plt.legend()
          #plt.savefig("distribution_attackers_episode{}_train-distrib.pdf".format(ep))
          #plt.show()
          #plt.clf()
      # Plot reward curve
    if (ep % 10 == 0):
      save_dir = '/scratch/ca2699/DDPG-MFG/0905_experiment_results/tune_lr'
      plt.plot([array[0] for array in ep_reward_list],label = 'Defenders reward')
      plt.plot([array[1] for array in ep_reward_list],label = 'Attackers reward')
      plt.xlabel("episode")
      plt.ylabel("reward")
      save_path = os.path.join(save_dir, "The_Second_rewards_episode{}.pdf".format(ep))
      plt.savefig(save_path)
      plt.show()
      plt.clf()
        # Plot smoothed reward curve
      N_mean = 20
        #print(ep_reward_list)
      cumsum_reward = np.cumsum(np.insert(ep_reward_list, 0, 0))
      mean_reward = (cumsum_reward[N_mean:] - cumsum_reward[:-N_mean])/float(N_mean)
        #print('the mean reward is',mean_reward)
      plt.plot(mean_reward)
      plt.xlabel("episode")
      plt.ylabel("mean reward")
      plt.show()
      plt.clf()

    #we consider the exploitability for every 4 episodes in the first 20 episodes, and for every 10 episodes for the episodes to come
    if (ep<= 20):
      if (ep%4==0):
        for i in range(len(actor_lr_lst_def)):
          actor_lr_def = actor_lr_lst_def[i]
          critic_lr_def = critic_lr_lst_def[i]
          actor_lr_att = actor_lr_lst_att[i]
          critic_lr_att = critic_lr_lst_att[i]
          for j in range(len(sdv_lst_def)):
              std_dev_ou_noise_def = sdv_lst_def[j]
              std_dev_ou_noise_att = sdv_lst_att[i]
              max_J_def = compute_maxJ(actor_model_def,critic_model_def,target_actor_def, target_critic_def, target_actor_att,
                                  gamma,
                                  training_env,
                                  ou_noise_zero,
                                  ep,
                                  distrib_init_sampler,
                                  distrib_init_sampler_from_list,
                                  distrib_init_list,
                                  actor_lr_def,
                                  critic_lr_def,
                                  std_dev_ou_noise_def,
                                  tau_def,
                                  eval_times = 11,
                                  object_index = 0, other = 1
                                  )
              max_J_att = compute_maxJ(actor_model_att,critic_model_att,target_actor_att, target_critic_att, target_actor_def,
                                  gamma,
                                  training_env,
                                  ou_noise_zero,
                                  ep,
                                  distrib_init_sampler,
                                  distrib_init_sampler_from_list,
                                  distrib_init_list,
                                  actor_lr_att,
                                  critic_lr_att,
                                  std_dev_ou_noise_att,
                                  tau_att,
                                  eval_times = 11,
                                  object_index = 1, other = 0
                                  )

              J_def, J_att = compute_J_i(target_actor_def, target_actor_att, training_env, distrib_init_sampler, distrib_init_sampler_from_list, distrib_init_list, ou_noise_zero, eval_times = 2)
              ep_max_J_def_list.append(max_J_def)
              ep_max_J_att_list.append(max_J_att)
              ep_J_def_list.append(J_def)
              ep_J_att_list.append(J_att)
              print('maxJ and J def is ', max_J_def, J_def)
              print('maxJ and J att is', max_J_att, J_att)
              ep_exploitability_def = max_J_def - J_def
              ep_exploitability_att = max_J_att - J_att
              ep_exploitability_list_def.append(ep_exploitability_def)
              ep_exploitability_list_att.append(ep_exploitability_att)
              ep_exploitability_list.append(ep_exploitability_def+ep_exploitability_att)
              print(ep_exploitability_list)

              #output_dir = '/scratch/ca2699/DDPG-MFG/logs'
              output_dir = '/scratch/ca2699/DDPG-MFG/0905_experiment_results/tune_lr/logs'
              output_filename = 'output2.csv'
              output_filepath = os.path.join(output_dir, output_filename)

              with open(output_filepath, 'w', newline='') as file:
                writer = csv.writer(file)
                writer.writerow(["defender exploitability", "attacker exploitability", "combined exploitability"])  # 写入表头

    # 写入多个数据点
                for def_exp, att_exp, comb_exp in zip(ep_exploitability_list_def, ep_exploitability_list_att, ep_exploitability_list):
                    writer.writerow([def_exp, att_exp, comb_exp])

              save_dir = '/scratch/ca2699/DDPG-MFG/0905_experiment_results/tune_lr'

              #decrease and converge to 0
              x_ticks = [i*4 for i in range(len(ep_exploitability_list_def))]
              plt.figure()
              plt.plot(x_ticks, ep_exploitability_list_def)
              plt.xlabel('episode')
              plt.ylabel('real exploitability')
              plt.title('Exploitability for cyber security exmple defenders')

              plt.tight_layout()
              #save_dir = 'logs/calculate_exploitability'
              #save_path = os.path.join(save_dir, 'exploitability_episode.pdf')
              #os.makedirs(save_dir, exist_ok=True)
              #plt.savefig(save_path)
              save_path = os.path.join(save_dir, 'The_Second_calculate_exploitability_exploitability_defenders_episode{}.pdf'.format(ep))
              plt.savefig(save_path)
              plt.show()
              plt.clf()
              print('this is the init population distribution list 8',distrib_init_list[0])
              x_ticks = [i*4 for i in range(len(ep_exploitability_list_att))]
              plt.figure()
              plt.plot(x_ticks, ep_exploitability_list_att)
              plt.xlabel('episode')
              plt.ylabel('real exploitability')
              plt.title('Exploitability for cyber security exmple for attackers')

              plt.tight_layout()
              #save_dir = 'logs/calculate_exploitability'
              #save_path = os.path.join(save_dir, 'exploitability_episode.pdf')
              #os.makedirs(save_dir, exist_ok=True)
              #plt.savefig(save_path)
              save_path = os.path.join(save_dir, 'The_Second_calculate_exploitability_exploitability_attackers_episode{}.pdf'.format(ep))
              plt.savefig(save_path)
              plt.show()
              plt.clf()
              print('this is the init population distribution list 8',distrib_init_list[0])
              x_ticks = [i*4 for i in range(len(ep_exploitability_list))]
              plt.figure()
              plt.plot(x_ticks, ep_exploitability_list)
              plt.xlabel('episode')
              plt.ylabel('real exploitability')
              plt.title('Exploitability for cyber security exmple')

              plt.tight_layout()
              #save_dir = 'logs/calculate_exploitability'
              #save_path = os.path.join(save_dir, 'exploitability_episode.pdf')
              #os.makedirs(save_dir, exist_ok=True)
              #plt.savefig(save_path)
              save_path = os.path.join(save_dir, 'The_Second_calculate_exploitability_exploitability_episode{}.pdf'.format(ep))
              plt.savefig(save_path)
              plt.show()
              plt.clf()
              print('this is the init population distribution list 8',distrib_init_list[0])
    elif (ep>20):
      if (ep%10==0):
        for i in range(len(actor_lr_lst_def)):
          actor_lr_def = actor_lr_lst_def[i]
          critic_lr_def = critic_lr_lst_def[i]
          actor_lr_att = actor_lr_lst_att[i]
          critic_lr_att = critic_lr_lst_att[i]
          for j in range(len(sdv_lst_def)):
              std_dev_ou_noise_def = sdv_lst_def[j]
              std_dev_ou_noise_att = sdv_lst_att[i]
              max_J_def = compute_maxJ(actor_model_def,critic_model_def,target_actor_def, target_critic_def, target_actor_att,
                                  gamma,
                                  training_env,
                                  ou_noise_zero,
                                  ep,
                                  distrib_init_sampler,
                                  distrib_init_sampler_from_list,
                                  distrib_init_list,
                                  actor_lr_def,
                                  critic_lr_def,
                                  std_dev_ou_noise_def,
                                  tau_def,
                                  eval_times = 11,
                                  object_index = 0, other = 1
                                  )
              max_J_att = compute_maxJ(actor_model_att,critic_model_att,target_actor_att, target_critic_att, target_actor_def,
                                  gamma,
                                  training_env,
                                  ou_noise_zero,
                                  ep,
                                  distrib_init_sampler,
                                  distrib_init_sampler_from_list,
                                  distrib_init_list,
                                  actor_lr_att,
                                  critic_lr_att,
                                  std_dev_ou_noise_att,
                                  tau_att,
                                  eval_times = 11,
                                  object_index = 1, other = 0
                                  )

              J_def, J_att = compute_J_i(target_actor_def, target_actor_att, training_env, distrib_init_sampler, distrib_init_sampler_from_list, distrib_init_list, ou_noise_zero, eval_times = 2)
              ep_max_J_def_list.append(max_J_def)
              ep_max_J_att_list.append(max_J_att)
              ep_J_def_list.append(J_def)
              ep_J_att_list.append(J_att)
              print('maxJ and J def is ', max_J_def, J_def)
              print('maxJ and J att is', max_J_att, J_att)
              ep_exploitability_def = max_J_def - J_def
              ep_exploitability_att = max_J_att - J_att
              ep_exploitability_list_def.append(ep_exploitability_def)
              ep_exploitability_list_att.append(ep_exploitability_att)
              ep_exploitability_list.append(ep_exploitability_def+ep_exploitability_att)
              print(ep_exploitability_list)

              #output_dir = '/scratch/ca2699/DDPG-MFG/logs'
              output_dir = '/scratch/ca2699/DDPG-MFG/0905_experiment_results/tune_lr/logs'
              output_filename = 'output2.csv'
              output_filepath = os.path.join(output_dir, output_filename)

              with open(output_filepath, 'w', newline='') as file:
                writer = csv.writer(file)
                writer.writerow(["defender exploitability", "attacker exploitability", "combined exploitability"])  # 写入表头

    # 写入多个数据点
                for def_exp, att_exp, comb_exp in zip(ep_exploitability_list_def, ep_exploitability_list_att, ep_exploitability_list):
                    writer.writerow([def_exp, att_exp, comb_exp])

              #save_dir = '/scratch/ca2699/DDPG-MFG/0905_experiment_results/Basic_test'

              #decrease and converge to 0
              #x_ticks = [i*2 for i in range(len(ep_exploitability_list_def))]
              x_ticks = []
              for i in range(5):
                x_ticks.append(i * 4)
              for i in range(5, len(ep_exploitability_list_def)):
                x_ticks.append(20 + (i - 5) * 10)
              plt.figure()
              plt.plot(x_ticks, ep_exploitability_list_def)
              plt.xlabel('episode')
              plt.ylabel('real exploitability')
              plt.title('Exploitability for cyber security exmple defenders')

              plt.tight_layout()
              #save_dir = 'logs/calculate_exploitability'
              #save_path = os.path.join(save_dir, 'exploitability_episode.pdf')
              #os.makedirs(save_dir, exist_ok=True)
              #plt.savefig(save_path)
              save_path = os.path.join(save_dir, 'The_Second_calculate_exploitability_exploitability_defenders_episode{}.pdf'.format(ep))
              plt.savefig(save_path)
              plt.show()
              plt.clf()
              print('this is the init population distribution list 8',distrib_init_list[0])
              #x_ticks = [i*2 for i in range(len(ep_exploitability_list_att))]
              plt.figure()
              plt.plot(x_ticks, ep_exploitability_list_att)
              plt.xlabel('episode')
              plt.ylabel('real exploitability')
              plt.title('Exploitability for cyber security exmple for attackers')

              plt.tight_layout()
              #save_dir = 'logs/calculate_exploitability'
              #save_path = os.path.join(save_dir, 'exploitability_episode.pdf')
              #os.makedirs(save_dir, exist_ok=True)
              #plt.savefig(save_path)
              save_path = os.path.join(save_dir, 'The_Second_calculate_exploitability_exploitability_attackers_episode{}.pdf'.format(ep))
              plt.savefig(save_path)
              plt.show()
              plt.clf()
              print('this is the init population distribution list 8',distrib_init_list[0])
              #x_ticks = [i*2 for i in range(len(ep_exploitability_list))]
              plt.figure()
              plt.plot(x_ticks, ep_exploitability_list)
              plt.xlabel('episode')
              plt.ylabel('real exploitability')
              plt.title('Exploitability for cyber security exmple')

              plt.tight_layout()
              #save_dir = 'logs/calculate_exploitability'
              #save_path = os.path.join(save_dir, 'exploitability_episode.pdf')
              #os.makedirs(save_dir, exist_ok=True)
              #plt.savefig(save_path)
              save_path = os.path.join(save_dir, 'The_First_calculate_exploitability_exploitability_episode{}.pdf'.format(ep))
              plt.savefig(save_path)
              plt.show()
              plt.clf()
              print('this is the init population distribution list 8',distrib_init_list[0])
    #print('this is episodic reward def',episodic_reward_def)
    #print('this is episodic reward att',episodic_reward_att)
    ep_reward_list_def.append(episodic_reward_def)
    ep_reward_list_att.append(episodic_reward_att)
    #ep_test_reward_list.append(episodic_test_reward)

    ep_reward = np.array([episodic_reward_def, episodic_reward_att])
    ep_reward_list.append(ep_reward)

    avg_reward_def = np.mean(ep_reward_list[-10:])
    avg_reward = np.array(avg_reward_def)




    # Mean of last 10 episodes
    avg_reward = np.mean(ep_reward_list[-10:])
    if (ep % 10 == 0):
      print("Episode * {} * Avg Reward is ==> {}".format(ep, avg_reward))
    avg_reward_list.append(avg_reward)

  #return ep_reward_list

def distrib_init_sampler_rand_uniform():
    v1 = np.random.rand(N_STATES_defenders)
    prob_v1 = list(v1/np.sum(v1))
    v2 = np.random.rand(N_STATES_attackers)
    prob_v2 = list(v2/np.sum(v2))
    return [prob_v1,prob_v2]

##@title Main loop


# ==================== MAIN LOOP ==================== #
# PARAMETERS
T_training = 10.0
dt_training = 0.1
gamma = 0.99

#tune the parameters
actor_lr_lst_def = [ 0.0006]
critic_lr_lst_def = [ 0.0009]
sdv_lst_def = [0.08]
actor_lr_lst_att = [0.00006]
critic_lr_lst_att = [0.00009]
sdv_lst_att = [0.08]

#distrib_init_list = [[[0.25, 0.25, 0.25, 0.25],[0,1]], [[1.0, 0.0, 0.0, 0.0],[0.1,0.9]], [[0.0, 0.0, 0.0, 1.0],[1.0,0.0]], [[0.4, 0.1, 0.4, 0.1],[0.5,0.5]], [[0.5, 0.2, 0.2, 0.1],[0.3,0.7]]]
distrib_init_list = [[[0.1, 0.7, 0.1, 0.1],[0.2,0.8]],[[0.25, 0.15, 0.35, 0.25],[0.55,0.45]], [[0.35, 0.35, 0.15, 0.15],[0.7,0.3]], [[0.5, 0.0, 0.5, 0.0],[0.55,0.45]], [[0.5, 0.2, 0.2, 0.1],[0.3,0.7]]]
distrib_init_sampler_from_list = lambda: distrib_init_list[np.random.randint(len(distrib_init_list))]
distrib_init_sampler_from_list = lambda: distrib_init_list[np.random.randint(len(distrib_init_list))]
distrib_init_sampler = lambda: distrib_init_sampler_rand_uniform()
def rand_unit_vec(dim):
  v = np.random.uniform(0.0, 1.0, size=dim)
  return v / np.sum(v)
#distrib_init_sampler_unif = lambda: rand_unit_vec(N_STATES)
#distrib_init_sampler = distrib_init_sampler_unif()
cn_sampler_zero = lambda x : 0
cn_sampler_gauss = lambda x : np.clip(x + np.random.normal(0.0,0.1), v_H_DEFAULT*0.5, v_H_DEFAULT*1.5)
cn_sampler = cn_sampler_gauss
env_tmp = CyberSecEnv(distrib_init_list[0], T_training, dt_training, cn_sampler)
num_states_def = env_tmp.observation_space_def.shape[0]
num_actions_def = env_tmp.action_space_defenders.shape[0]
num_states_att = env_tmp.observation_space_att.shape[0]
num_actions_att = env_tmp.action_space_attackers.shape[0]
print("num_states_defenders = ", num_states_def, "num_states_attackers = ", num_states_att,
          "num_actions_defenders = ", num_actions_def," num_actions_attackers = ", num_actions_att)

# Create networks
actor_model_def = get_actor_def(num_states_def=N_STATES_defenders,num_states_att=N_STATES_attackers,num_actions=N_STATES_defenders)
critic_model_def = get_critic(num_states_def=N_STATES_defenders,num_states_att=N_STATES_attackers,num_actions=N_STATES_defenders)
target_actor_def = get_actor_def(num_states_def=N_STATES_defenders,num_states_att=N_STATES_attackers,num_actions=N_STATES_defenders)
target_critic_def = get_critic(num_states_def=N_STATES_defenders,num_states_att=N_STATES_attackers,num_actions=N_STATES_defenders)

actor_model_att = get_actor_att(num_states_def=N_STATES_defenders,num_states_att=N_STATES_attackers,num_actions=N_STATES_attackers)
critic_model_att = get_critic(num_states_def=N_STATES_defenders,num_states_att=N_STATES_attackers,num_actions=N_STATES_attackers)
target_actor_att = get_actor_att(num_states_def=N_STATES_defenders,num_states_att=N_STATES_attackers,num_actions=N_STATES_attackers)
target_critic_att = get_critic(num_states_def=N_STATES_defenders,num_states_att=N_STATES_attackers,num_actions=N_STATES_attackers)

# Making the weights equal initially
target_actor_def.set_weights(actor_model_def.get_weights())
target_critic_def.set_weights(critic_model_def.get_weights())

target_actor_att.set_weights(actor_model_att.get_weights())
target_critic_att.set_weights(critic_model_att.get_weights())

# Learning rate for actor-critic models
'''critic_lr_def = 0.00009
actor_lr_def = 0.00006
critic_lr_att = 0.00009
actor_lr_att = 0.00006'''

critic_lr_def = 0.0009
actor_lr_def = 0.0006
critic_lr_att = 0.00009
actor_lr_att = 0.00006

critic_optimizer_def = tf.keras.optimizers.Adam(critic_lr_def)
actor_optimizer_def = tf.keras.optimizers.Adam(actor_lr_def)
critic_optimizer_att = tf.keras.optimizers.Adam(critic_lr_att)
actor_optimizer_att = tf.keras.optimizers.Adam(actor_lr_att)

total_episodes = 300
# Used to update target networks
tau_def = 0.005
tau_att = 0.0005
# TRAIN POLICY
env_training = CyberSecEnv(distrib_init_list[0], T_training, dt_training, cn_sampler_zero)
buffer = Buffer(num_states = 4, num_actions = 2, buffer_capacity=5000,batch_size=64)
train_policy(env_training, actor_model_def, actor_optimizer_def, critic_model_def, critic_optimizer_def, target_actor_def, target_critic_def,
                 actor_model_att, actor_optimizer_att, critic_model_att, critic_optimizer_att, target_actor_att, target_critic_att,
                 gamma, tau_def, tau_att, buffer, total_episodes, distrib_init_sampler, distrib_init_sampler_from_list, distrib_init_list, actor_lr_lst_def,critic_lr_lst_def,sdv_lst_def, actor_lr_lst_att, critic_lr_lst_att, sdv_lst_att)
